{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31b8f1a9",
   "metadata": {},
   "source": [
    "\n",
    "# Scrapy Code Examples — From Basics to Useful Patterns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a868e5",
   "metadata": {},
   "source": [
    "\n",
    "## How to Use\n",
    "- `%%bash` cells are terminal commands (kept here for convenience).\n",
    "- Python cells show minimal patterns, swap selectors/URLs for your site.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c97483c",
   "metadata": {},
   "source": [
    "## 0) Install / Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2aace28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%bash\n",
    "# Install Scrapy (uncomment to use). Prefer virtualenv or conda.\n",
    "# python -m pip install --upgrade pip\n",
    "# pip install scrapy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b81ae9",
   "metadata": {},
   "source": [
    "## 1) Start a Project (once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e67e63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%bash\n",
    "# Create a standard Scrapy project.\n",
    "# scrapy startproject myproject\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b9b5dd",
   "metadata": {},
   "source": [
    "\n",
    "### Project Layout (reference)\n",
    "```\n",
    "myproject/\n",
    "├─ myproject/\n",
    "│  ├─ spiders/\n",
    "│  ├─ items.py\n",
    "│  ├─ pipelines.py\n",
    "│  ├─ settings.py\n",
    "│  └─ __init__.py\n",
    "└─ scrapy.cfg\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5009ec18",
   "metadata": {},
   "source": [
    "## 2) Minimal Spider (single page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b154f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%bash\n",
    "mkdir -p myproject/myproject/spiders\n",
    "cat > myproject/myproject/spiders/minimal_spider.py << 'PY'\n",
    "import scrapy\n",
    "\n",
    "class MinimalSpider(scrapy.Spider):\n",
    "    name = \"minimal_example\"\n",
    "    allowed_domains = [\"example.com\"]  # optional safety net\n",
    "    start_urls = [\"https://example.com\"]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for card in response.css(\"div.card\"):\n",
    "            yield {\n",
    "                \"title\": card.css(\"h2::text\").get(),\n",
    "                \"link\": response.urljoin(card.css(\"a::attr(href)\").get()),\n",
    "            }\n",
    "PY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a820b06f",
   "metadata": {},
   "source": [
    "\n",
    "**Run (terminal):**\n",
    "```bash\n",
    "cd myproject\n",
    "scrapy crawl minimal_example -O minimal.csv\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35d628a",
   "metadata": {},
   "source": [
    "## 3) Selectors Cheat (copy & tweak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584153b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Inside parse():\n",
    "# CSS:\n",
    "# response.css(\"article.product_pod h3 a::attr(title)\").get()\n",
    "# response.css(\".price_color::text\").get()\n",
    "# response.css(\"li.next a::attr(href)\").get()\n",
    "#\n",
    "# XPath:\n",
    "# response.xpath(\"//article[contains(@class,'product_pod')]//h3/a/@title\").get()\n",
    "# response.xpath(\"//p[@class='price_color']/text()\").get()\n",
    "# response.xpath(\"//li[@class='next']/a/@href\").get()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c2b950",
   "metadata": {},
   "source": [
    "## 4) Pagination Spider (follow 'Next')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d026c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%bash\n",
    "cat > myproject/myproject/spiders/pagination_spider.py << 'PY'\n",
    "import scrapy\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "class PaginationSpider(scrapy.Spider):\n",
    "    name = \"pagination_example\"\n",
    "    start_urls = [\"https://books.toscrape.com/\"]\n",
    "\n",
    "    custom_settings = {\n",
    "        \"FEEDS\": {\"pagination_books.csv\": {\"format\": \"csv\", \"overwrite\": True}},\n",
    "        \"ROBOTSTXT_OBEY\": True,\n",
    "        \"DOWNLOAD_DELAY\": 0.5,\n",
    "        \"AUTOTHROTTLE_ENABLED\": True,\n",
    "        \"USER_AGENT\": \"Scrapy-Examples (+https://example.org/edu)\",\n",
    "    }\n",
    "\n",
    "    def parse(self, response):\n",
    "        for product in response.css(\"article.product_pod\"):\n",
    "            yield {\n",
    "                \"title\": product.css(\"h3 a::attr(title)\").get(),\n",
    "                \"price\": product.css(\".price_color::text\").get(),\n",
    "                \"url\": response.urljoin(product.css(\"h3 a::attr(href)\").get()),\n",
    "            }\n",
    "        next_rel = response.css(\"li.next a::attr(href)\").get()\n",
    "        if next_rel:\n",
    "            yield response.follow(urljoin(response.url, next_rel), callback=self.parse)\n",
    "PY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84e01c1",
   "metadata": {},
   "source": [
    "\n",
    "**Run (terminal):**\n",
    "```bash\n",
    "cd myproject\n",
    "scrapy crawl pagination_example\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d603aab9",
   "metadata": {},
   "source": [
    "## 5) List → Detail Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d27f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%bash\n",
    "cat > myproject/myproject/spiders/list_detail_spider.py << 'PY'\n",
    "import scrapy\n",
    "\n",
    "class ListDetailSpider(scrapy.Spider):\n",
    "    name = \"list_detail_example\"\n",
    "    start_urls = [\"https://books.toscrape.com/\"]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for product in response.css(\"article.product_pod\"):\n",
    "            detail_url = response.urljoin(product.css(\"h3 a::attr(href)\").get())\n",
    "            yield response.follow(detail_url, callback=self.parse_detail, meta={\n",
    "                \"title\": product.css(\"h3 a::attr(title)\").get(),\n",
    "                \"price\": product.css(\".price_color::text\").get(),\n",
    "            })\n",
    "\n",
    "        next_rel = response.css(\"li.next a::attr(href)\").get()\n",
    "        if next_rel:\n",
    "            yield response.follow(next_rel, callback=self.parse)\n",
    "\n",
    "    def parse_detail(self, response):\n",
    "        item = {\n",
    "            \"title\": response.meta.get(\"title\"),\n",
    "            \"price\": response.meta.get(\"price\"),\n",
    "            \"url\": response.url,\n",
    "        }\n",
    "        desc = response.css(\"#product_description ~ p::text\").get()\n",
    "        item[\"description\"] = desc.strip() if desc else None\n",
    "        yield item\n",
    "PY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698a0bc4",
   "metadata": {},
   "source": [
    "## 6) Items & Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2289aa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%bash\n",
    "cat > myproject/myproject/items.py << 'PY'\n",
    "import scrapy\n",
    "\n",
    "class ProductItem(scrapy.Item):\n",
    "    title = scrapy.Field()\n",
    "    price = scrapy.Field()\n",
    "    url = scrapy.Field()\n",
    "    rating = scrapy.Field()\n",
    "    rating_num = scrapy.Field()\n",
    "    available = scrapy.Field()\n",
    "PY\n",
    "\n",
    "cat > myproject/myproject/pipelines.py << 'PY'\n",
    "import re\n",
    "\n",
    "class CleanPipeline:\n",
    "    price_re = re.compile(r\"[\\d.]+\")\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        # price -> float\n",
    "        m = self.price_re.search((item.get(\"price\") or \"\"))\n",
    "        item[\"price\"] = float(m.group(0)) if m else None\n",
    "\n",
    "        # rating words -> numbers\n",
    "        rating_map = {\"One\":1,\"Two\":2,\"Three\":3,\"Four\":4,\"Five\":5}\n",
    "        item[\"rating_num\"] = rating_map.get(item.get(\"rating\"))\n",
    "\n",
    "        # available flag (example)\n",
    "        avail = (item.get(\"available\") or item.get(\"availability\") or \"\").lower()\n",
    "        item[\"available\"] = \"in stock\" in avail\n",
    "        return item\n",
    "PY\n",
    "\n",
    "python - << 'PY'\n",
    "from pathlib import Path\n",
    "p = Path(\"myproject/myproject/settings.py\")\n",
    "txt = p.read_text(encoding=\"utf-8\")\n",
    "if \"ITEM_PIPELINES\" not in txt:\n",
    "    txt += \"\\n\\nITEM_PIPELINES = {\\n    'myproject.pipelines.CleanPipeline': 300,\\n}\\n\"\n",
    "p.write_text(txt, encoding=\"utf-8\")\n",
    "print(\"Enabled CleanPipeline in settings.py\")\n",
    "PY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8920168f",
   "metadata": {},
   "source": [
    "## 7) Export Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f594c367",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CLI:\n",
    "# scrapy crawl <name> -O out.csv     # overwrite\n",
    "# scrapy crawl <name> -O out.jsonl   # JSON lines\n",
    "#\n",
    "# settings.py (global):\n",
    "# FEEDS = {\n",
    "#   'output/items.csv': {'format': 'csv', 'overwrite': True},\n",
    "#   'output/items.jsonl': {'format': 'jsonlines'}\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7f43bc",
   "metadata": {},
   "source": [
    "## 8) Settings Essentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01eed19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%bash\n",
    "python - << 'PY'\n",
    "from pathlib import Path\n",
    "p = Path(\"myproject/myproject/settings.py\")\n",
    "txt = p.read_text(encoding=\"utf-8\")\n",
    "\n",
    "additions = [\n",
    "    \"ROBOTSTXT_OBEY = True\",\n",
    "    \"DOWNLOAD_DELAY = 0.5\",\n",
    "    \"AUTOTHROTTLE_ENABLED = True\",\n",
    "    \"USER_AGENT = 'Scrapy-Examples (+https://example.org/edu)'\",\n",
    "    \"RETRY_ENABLED = True\",\n",
    "    \"RETRY_TIMES = 2\",\n",
    "    # \"LOG_LEVEL = 'INFO'\",\n",
    "]\n",
    "for line in additions:\n",
    "    if line.split(\"=\")[0].strip() not in txt:\n",
    "        txt += \"\\n\" + line\n",
    "p.write_text(txt, encoding=\"utf-8\")\n",
    "print(\"Updated settings.py with essentials (idempotent).\")\n",
    "PY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdfcd69",
   "metadata": {},
   "source": [
    "## 9) Scrapy Shell (validate selectors fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb008ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%bash\n",
    "# Usually run in terminal:\n",
    "# scrapy shell https://books.toscrape.com/\n",
    "# response.css(\"article.product_pod h3 a::attr(title)\").getall()\n",
    "# response.css(\"li.next a::attr(href)\").get()\n",
    "# response.xpath(\"//p[@class='price_color']/text()\").get()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dd21cd",
   "metadata": {},
   "source": [
    "## 10) Programmatic Run (template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c410b399",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run Scrapy without CLI — template (copy to a .py script inside project):\n",
    "# ---------------------------------------------------------\n",
    "# from scrapy.crawler import CrawlerProcess\n",
    "# from scrapy.utils.project import get_project_settings\n",
    "# from myproject.spiders.pagination_spider import PaginationSpider\n",
    "#\n",
    "# process = CrawlerProcess(get_project_settings())\n",
    "# process.crawl(PaginationSpider)\n",
    "# process.start()\n",
    "# ---------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f8c268",
   "metadata": {},
   "source": [
    "## 11) Checklist to Adapt to Any Site"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb232094",
   "metadata": {},
   "source": [
    "\n",
    "1. Identify target pages and **start URLs**.  \n",
    "2. Use **Scrapy Shell** to perfect selectors.  \n",
    "3. Build a **minimal spider** that yields a few fields.  \n",
    "4. Add **pagination** (follow 'next').  \n",
    "5. Add optional **detail page** parsing for extra fields.  \n",
    "6. Enable **FEEDS** and verify output.  \n",
    "7. Add **pipelines** for cleaning/validation.  \n",
    "8. Configure **politeness** and **retries** in settings.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
